{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d6b6b3-b1e5-42c2-8db6-46130c0784f8",
   "metadata": {},
   "source": [
    "# Demonstrate Pytorch Lightning on Jupyter\n",
    "\n",
    "**IMPORTANT**: must set `strategy=None|dp`.\n",
    "- Contrary to the lightning's error message, we found that `strategy=None` does not work.\n",
    "- Related issue: https://github.com/Lightning-AI/lightning/issues/7550\n",
    "\n",
    "This means that on Jupyter, multi-GPU works but not multi-Gaudi because Gaudi's pytorch does not\n",
    "support DP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae38f984-3ade-44dd-8b85-1e45de20245b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "from bench import Text8Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a517f180-90fd-4567-ac52-a5a31e172069",
   "metadata": {},
   "source": [
    "## Training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4a4eea-bf26-4240-8ae3-9686591c816d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.num_epochs = 5\n",
    "args.batch_size = 64\n",
    "args.block_size = 128\n",
    "args.num_workers = 0\n",
    "args.pin_memory = 0\n",
    "args.precision = 16\n",
    "args.default_root_dir = \".\"\n",
    "print(vars(args))\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61395c6d-2a4b-4dbe-9825-08fcf86f1e8a",
   "metadata": {},
   "source": [
    "## Prepare data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798dbb1e-0f32-4aeb-a441-9edbd2019613",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(\"preparing the data loaders\")\n",
    "# NOTE: REDUCED DATA SIZE FOR DEBUGGING, TODO CLEAN BEFORE MERGE IF EVER\n",
    "train_dataset = Text8Dataset(\"../data/text8\", args.block_size, crop=(0, int(90e4)))\n",
    "val_dataset = Text8Dataset(\n",
    "    \"../data/text8\",\n",
    "    args.block_size,\n",
    "    crop=(int(90e4), int(5e4)),\n",
    "    override_vocab=train_dataset.vocab,\n",
    "    unknown_ch2i=0,\n",
    ")\n",
    "test_dataset = Text8Dataset(\n",
    "    \"../data/text8\",\n",
    "    args.block_size,\n",
    "    crop=(int(95e4), int(5e4)),\n",
    "    override_vocab=train_dataset.vocab,\n",
    "    unknown_ch2i=0,\n",
    ")\n",
    "common = {\n",
    "    \"batch_size\": args.batch_size,\n",
    "    \"pin_memory\": bool(args.pin_memory),\n",
    "    \"num_workers\": args.num_workers,\n",
    "}\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, **common)\n",
    "val_dataloader = DataLoader(val_dataset, shuffle=False, **common)\n",
    "\n",
    "logging.info(\"creating the model\")\n",
    "model = GPT(train_dataset.vocab_size, args.block_size, n_layer=8, n_head=8, n_embd=256)\n",
    "\n",
    "logging.info(\"preparing the learning rate schedule\")\n",
    "iter_tokens = args.batch_size * args.block_size  # number of tokens backpropped in one iteration\n",
    "epoch_tokens = math.ceil(len(train_dataset) / args.batch_size) * iter_tokens\n",
    "lr_decay = WarmupCosineLearningRateDecay(\n",
    "    learning_rate=6e-4,\n",
    "    warmup_tokens=epoch_tokens // 2,\n",
    "    final_tokens=args.num_epochs * epoch_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33b96f8-732e-482d-9973-f973b7684fb6",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba626b6d-10e2-4c9e-bdc3-b4f66c476edb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "logging.info(\"training...\")\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    benchmark=True,\n",
    "    max_epochs=args.num_epochs,\n",
    "    gradient_clip_val=1.0,\n",
    "    callbacks=[lr_decay, pl.callbacks.ModelSummary(max_depth=2)],\n",
    "    precision=args.precision,\n",
    "    default_root_dir=args.default_root_dir,\n",
    "    strategy=\"dp\",\n",
    ")\n",
    "trainer.fit(model, train_dataloader, val_dataloader)\n",
    "t1 = time.time()\n",
    "logging.info(\n",
    "    \"%d epochs took %fs, or %fs/epoch\", args.num_epochs, t1 - t0, (t1 - t0) / args.num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5a6c6-e79e-46c5-82f0-d3742bb9871d",
   "metadata": {},
   "source": [
    "## Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d0747d-0a78-49d2-a0aa-e7737270ca19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(\"testing...\")\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, **common)\n",
    "trainer.test(dataloaders=test_dataloader)\n",
    "\n",
    "logging.info(\"sampling:\")\n",
    "context = \"anarchism originated as a term of\"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None, ...]\n",
    "if next(model.parameters()).is_cuda:\n",
    "    x = x.cuda()\n",
    "y = sample(model, x, 200, temperature=1.0, sample=True, top_k=None)[0]\n",
    "completion = \"\".join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04852c93-17de-4597-88db-b490eaf59d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
