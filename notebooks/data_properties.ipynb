{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import my_nb_path  # isort: skip\n",
    "import os\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "import a2rl as wi\n",
    "from a2rl.nbtools import pprint, print  # Enable color outputs when rich is installed.\n",
    "from a2rl.utils import (\n",
    "    NotMDPDataError,\n",
    "    assert_mdp,\n",
    "    data_generator_gym,\n",
    "    data_generator_simple,\n",
    "    plot_information,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For many sequential decision making problems we look for some key patterns in the data\n",
    "\n",
    "* Markov property\n",
    "\n",
    "* A consistent reward or cost\n",
    "\n",
    "* Actions being effective in contributing to the reward or affecting the Environment\n",
    "\n",
    "* Seeing if there is a consistent way that actions are picked\n",
    "\n",
    "\n",
    "We have a few helper visualisations to help these are markovian_matrix and normalized_markovian_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection\n",
    "\n",
    "In the offline setting we are restricted only to data. `whatif` offers three ways to generate some:\n",
    "\n",
    "1. The load-and-discretize workflow <- The main one. See `discretized_sample_dataset()`.\n",
    "\n",
    "2. `data_generator_gym` to load data interations between a trained agent and a gym environment <- This is for testing and research\n",
    "\n",
    "3. `data_generator_simple` to generate sample data with different properties <- Also for testing and research\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretized_sample_dataset(dataset_name: str, n_bins=50) -> wi.WiDataFrame:\n",
    "    \"\"\"Discretized a sample dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name: name of the sample dataset.\n",
    "\n",
    "    Returns:\n",
    "        Whatif dataframe.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    list_sample_datasets\n",
    "    \"\"\"\n",
    "    dirname = wi.sample_dataset_path(dataset_name)\n",
    "    tokeniser = wi.DiscreteTokenizer(n_bins=n_bins)\n",
    "    df = tokeniser.fit_transform(wi.read_csv_dataset(dirname))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = 10  # Same as assert_mdp()'s default.\n",
    "\n",
    "################################################################################\n",
    "# To run in fast mode, set env var NOTEBOOK_FAST_RUN=1 prior to starting Jupyter\n",
    "################################################################################\n",
    "if os.environ.get(\"NOTEBOOK_FAST_RUN\", \"0\") != \"0\":\n",
    "    lags = 5\n",
    "    display(\n",
    "        Markdown(\n",
    "            '<p style=\"color:firebrick; background-color:yellow; font-weight:bold\">'\n",
    "            \"NOTE: notebook runs in fast mode. Use only 5 lags. Results may differ.\"\n",
    "        )\n",
    "    )\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Markov property and then add random actions (random policy) that affect the states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "offline_data = data_generator_simple(\n",
    "    markov_order=1,\n",
    "    reward_function=False,\n",
    "    action_effect=True,\n",
    "    policy=False,\n",
    ")\n",
    "\n",
    "try:\n",
    "    assert_mdp(offline_data, lags=lags)\n",
    "except NotMDPDataError as e:\n",
    "    print(\"Continue this example despite MDP check errors:\\n\", e)\n",
    "\n",
    "plot_information(offline_data, lags=lags);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use higher order Markov property and effective actions, and add a reward function that is related to the state and action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbsphinx-thumbnail"
    ]
   },
   "outputs": [],
   "source": [
    "offline_data = data_generator_simple(\n",
    "    markov_order=2,\n",
    "    reward_function=True,\n",
    "    action_effect=True,\n",
    "    policy=False,\n",
    ")\n",
    "\n",
    "try:\n",
    "    assert_mdp(offline_data, lags=lags)\n",
    "except NotMDPDataError as e:\n",
    "    print(\"Continue this example despite MDP check errors:\\n\", e)\n",
    "\n",
    "plot_information(offline_data, lags=lags);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAi gym environment with known MDP\n",
    "\n",
    "Use an agent that is not trained very much on Taxi dataset and see how's the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "gym_data = data_generator_gym(\n",
    "    env_name=\"Taxi-v3\",\n",
    "    trainer=DQN,\n",
    "    training_steps=10000,\n",
    "    capture_steps=100,\n",
    ")\n",
    "\n",
    "try:\n",
    "    assert_mdp(offline_data, lags=lags)\n",
    "except NotMDPDataError as e:\n",
    "    print(\"Continue this example despite MDP check errors:\\n\", e)\n",
    "\n",
    "plot_information(gym_data, lags=lags);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chiller Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_chiller = discretized_sample_dataset(\"chiller\", n_bins=10)\n",
    "try:\n",
    "    assert_mdp(df_chiller, lags=lags)\n",
    "except NotMDPDataError as e:\n",
    "    print(\"Continue this example despite MDP check errors:\\n\", e)\n",
    "\n",
    "plot_information(df_chiller, lags=lags);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
